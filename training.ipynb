{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d5352a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"dataset02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12072e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import yaml\n",
    "\n",
    "def load_class_names(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    # Handle both list or dict formats\n",
    "    if isinstance(data['names'], dict):\n",
    "        class_names = [data['names'][i] for i in sorted(data['names'])]\n",
    "    else:\n",
    "        class_names = data['names']\n",
    "    return class_names\n",
    "\n",
    "IMAGE_DIR = f'{dataset}/val/images'\n",
    "LABEL_DIR = f'{dataset}/val/labels'\n",
    "\n",
    "YAML_PATH = f'{dataset}/data.yaml'  # or wherever your .yaml file is\n",
    "CLASS_NAMES = load_class_names(YAML_PATH)\n",
    "print(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = defaultdict(int)\n",
    "class_examples = defaultdict(list)\n",
    "\n",
    "# Parse annotations and collect counts + examples\n",
    "for label_file in os.listdir(LABEL_DIR):\n",
    "    if not label_file.endswith('.txt'):\n",
    "        continue\n",
    "    label_path = os.path.join(LABEL_DIR, label_file)\n",
    "    image_name = os.path.splitext(label_file)[0] + '.jpg'\n",
    "    image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            cls_id = int(line.strip().split()[0])\n",
    "            class_counts[cls_id] += 1\n",
    "            if len(class_examples[cls_id]) < 3:\n",
    "                class_examples[cls_id].append((image_path, line.strip()))\n",
    "\n",
    "total_cones = 0\n",
    "for c in class_counts: \n",
    "    total_cones += class_counts[c] \n",
    "\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    print(f\"Class {i} - {cls}: {class_counts[i]} occurences ({round(class_counts[i]/total_cones, 2)*100}%)\")\n",
    "\n",
    "print(f\"Total: {total_cones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60caf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 1â€“3 examples for each class\n",
    "for cls_id, examples in class_examples.items():\n",
    "    print(f\"Class {cls_id} ({CLASS_NAMES[cls_id]}): {class_counts[cls_id]} occurrences\")\n",
    "    for img_path, annotation in examples:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        _, x, y, bw, bh = map(float, annotation.split())\n",
    "        x1 = int((x - bw / 2) * w)\n",
    "        y1 = int((y - bh / 2) * h)\n",
    "        x2 = int((x + bw / 2) * w)\n",
    "        y2 = int((y + bh / 2) * h)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "        cv2.putText(img, CLASS_NAMES[cls_id], (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Class {cls_id} Example\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov10n.yaml\")\n",
    "\n",
    "# Train with small batch size and limited workers\n",
    "results = model.train(\n",
    "    data='dataset02/data.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=2000,    \n",
    "    batch=2,\n",
    "    workers=0,         # Avoid multiprocessing overhead\n",
    "    device=0           # Make sure it uses the GPU\n",
    ")\n",
    "\n",
    "model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ccb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val(data=\"dataset02/data.yaml\", split='test')  # uses test images\n",
    "\n",
    "# Optional: print metrics\n",
    "print(metrics.box.map)      # mAP@0.5:0.95\n",
    "print(metrics.box.map50)    # mAP@0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.box.p)\n",
    "print(metrics.box.r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
